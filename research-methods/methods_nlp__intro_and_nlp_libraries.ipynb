{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3NCj8rkQMsdx"
   },
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "To be a brief comparison to packages that provide more generic NLP functionality. \n",
    "\n",
    "Except that these days there are quite a few, and we can only point out there is no shortage of choices,\n",
    "and that this is not an endorsement of any - it ends up depending on your purposes.\n",
    "\n",
    "Choices include, in no particular order:\n",
    "\n",
    "- [spacy](https://spacy.io/usage/spacy-101) - see also our own [methods_nlp__spacy_basics](methods_nlp__spacy_basics.ipynb)\n",
    "  - ~two dozen languages, including Dutch\n",
    "  - this one seems like a convenient generic choice to us, yet most of the things it does are also done - sometimes better - by other choices\n",
    "\n",
    "- [pattern](https://github.com/clips/pattern)\n",
    "  - 6 languages, including Dutch\n",
    "\n",
    "- [flair](https://github.com/flairNLP/flair)\n",
    "  - 4 languages, including Dutch\n",
    "\n",
    "- [stanza](https://stanfordnlp.github.io/stanza/)\n",
    "  - ~80 languages, including Dutch\n",
    "  - and note there is e.g. a [spacy-stanza](https://spacy.io/universe/project/spacy-stanza/)\n",
    "\n",
    "- [UDPipe](https://lindat.mff.cuni.cz/services/udpipe/)\n",
    "  - various languages, including Dutch\n",
    "  - and note there is a [spacy-udpipe](https://spacy.io/universe/project/spacy-udpipe)\n",
    "\n",
    "- [alpino](https://www.let.rug.nl/vannoord/alp/Alpino/)\n",
    "  - and note there is e.g. a [spacy-alpino](https://pypi.org/project/spacy-alpino/)\n",
    "\n",
    "- [trankit](https://github.com/nlp-uoregon/trankit)\n",
    "  - 50+ languages, including Dutch \n",
    "  - and note there is e.g. a [spacy-trankit](https://pypi.org/project/spacy-trankit/)\n",
    "\n",
    "- [nltk](https://www.nltk.org/)\n",
    "  - different components seem to support different languages, Dutch is not supported in all of them out-of-the-box\n",
    "  - though [training basic support is relatively simple](https://stackoverflow.com/questions/40212895/nltk-tag-dutch-sentence)\n",
    "\n",
    "- [textblob](https://textblob.readthedocs.io/en/dev/) \n",
    "  - language-extensible, but out of the box it seems to focus just on English\n",
    "  - so no Dutch support, though [there's this](https://github.com/gvisniuc/textblob-nl)\n",
    "\n",
    "- [CoreNLP](https://stanfordnlp.github.io/CoreNLP/)\n",
    "  - 8 languages, no Dutch\n",
    "\n",
    "- [NLP-Cube](https://github.com/Adobe/NLP-Cube)\n",
    "  - 8 languags (?), no Dutch?\n",
    "\n",
    "...in part just to mention them, in part to help you choose one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide when you need these - and when you may _not_ need these\n",
    "\n",
    "There are also various tasks that are automated, \n",
    "or common enough that there are plenty of interactive tools, from widgets as code, to complete no-code solutions.\n",
    "\n",
    "Data annotation is a good example of this. You want documents going in one side,\n",
    "annotation data coming in the other side, and a quick web search for purely-online annotation \n",
    "reveals tools like [label studio](https://labelstud.io/), [docanno](https://doccano.github.io/doccano/),\n",
    "[ML-annotate](https://github.com/falcony-io/ml-annotate), [brat](http://brat.nlplab.org/), [annotator.js](http://annotatorjs.org/)\n",
    "to more purpose-specific projectss such as [lawnotation](https://www.lawnotation.org/).\n",
    "\n",
    "The extent of your questions come down to \n",
    "- \"after I do a lot of clicks, how is the thing it spits out usable to me?\",\n",
    "- \"is this particular tool already aware of the language and scope I'm working in, and does it try to help me along?\"\n",
    "\n",
    "and specifically _not_\n",
    "- \"what do I need to install\"\n",
    "- \"what do I need to learn to even get started, in terms of programming, how your your package works, whether the output is what I need in the first place\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "test = \"Python is a high-level, general-purpose programming language. It can be quite useful.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some minimal examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qKsX4tkxMsd0"
   },
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Python', 'PROPN'),\n",
      "  ('is', 'AUX'),\n",
      "  ('a', 'DET'),\n",
      "  ('high', 'ADJ'),\n",
      "  ('-', 'PUNCT'),\n",
      "  ('level', 'NOUN'),\n",
      "  (',', 'PUNCT'),\n",
      "  ('general', 'ADJ'),\n",
      "  ('-', 'PUNCT'),\n",
      "  ('purpose', 'NOUN'),\n",
      "  ('programming', 'NOUN'),\n",
      "  ('language', 'NOUN'),\n",
      "  ('.', 'PUNCT'),\n",
      "  ('It', 'PRON'),\n",
      "  ('can', 'AUX'),\n",
      "  ('be', 'AUX'),\n",
      "  ('quite', 'ADV'),\n",
      "  ('useful', 'ADJ'),\n",
      "  ('.', 'PUNCT')],\n",
      " [Python is a high-level, general-purpose programming language.,\n",
      "  It can be quite useful.]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "english_lg  = spacy.load('en_core_web_lg')   \n",
    "\n",
    "ana = english_lg( test )\n",
    "\n",
    "pprint.pprint(\n",
    "    [list( (tok.text, tok.pos_)  for tok in ana ),\n",
    "     list(ana.sents)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textblob\n",
    "\n",
    "See also https://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "ana = TextBlob( test )\n",
    "\n",
    "pprint.pprint(\n",
    "    [ana.tags,\n",
    "     ana.sentences]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Python', 'NNP'),\n",
      "  ('is', 'VBZ'),\n",
      "  ('a', 'DT'),\n",
      "  ('high-level', 'JJ'),\n",
      "  ('general-purpose', 'JJ'),\n",
      "  ('programming', 'NN'),\n",
      "  ('language', 'NN'),\n",
      "  ('It', 'PRP'),\n",
      "  ('can', 'MD'),\n",
      "  ('be', 'VB'),\n",
      "  ('quite', 'RB'),\n",
      "  ('useful', 'JJ')],\n",
      " [Sentence(\"Python is a high-level, general-purpose programming language.\"),\n",
      "  Sentence(\"It can be quite useful.\")]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Python',\n",
      "  'is',\n",
      "  'a',\n",
      "  'high-level',\n",
      "  ',',\n",
      "  'general-purpose',\n",
      "  'programming',\n",
      "  'language',\n",
      "  '.',\n",
      "  'It',\n",
      "  'can',\n",
      "  'be',\n",
      "  'quite',\n",
      "  'useful',\n",
      "  '.'],\n",
      " ['Python is a high-level, general-purpose programming language.',\n",
      "  'It can be quite useful.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "pprint.pprint(\n",
    "    [word_tokenize(test), \n",
    "     sent_tokenize(test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "ana = parse(test,\n",
    "     tokenize = True,  \n",
    "         tags = True,  \n",
    "       chunks = True,  \n",
    "    relations = True,  \n",
    "      #lemmata = True,  \n",
    "        light = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Python', 'NNP', 'B-NP', 'O', 'NP-SBJ-1'],\n",
      "  ['is', 'VBZ', 'B-VP', 'O', 'VP-1'],\n",
      "  ['a', 'DT', 'B-NP', 'O', 'NP-OBJ-1'],\n",
      "  ['high-level', 'JJ', 'I-NP', 'O', 'NP-OBJ-1'],\n",
      "  [',', ',', 'I-NP', 'O', 'NP-OBJ-1'],\n",
      "  ['general-purpose', 'JJ', 'I-NP', 'O', 'NP-OBJ-1'],\n",
      "  ['programming', 'NN', 'I-NP', 'O', 'NP-OBJ-1'],\n",
      "  ['language', 'NN', 'I-NP', 'O', 'NP-OBJ-1'],\n",
      "  ['.', '.', 'O', 'O', 'O']],\n",
      " [['It', 'PRP', 'B-NP', 'O', 'NP-SBJ-1'],\n",
      "  ['can', 'MD', 'B-VP', 'O', 'VP-1'],\n",
      "  ['be', 'VB', 'I-VP', 'O', 'VP-1'],\n",
      "  ['quite', 'RB', 'B-ADJP', 'O', 'O'],\n",
      "  ['useful', 'JJ', 'I-ADJP', 'O', 'O'],\n",
      "  ['.', '.', 'O', 'O', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint( ana.split() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreNLP\n",
    "\n",
    "See also https://stanfordnlp.github.io/CoreNLP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 15:14:35,772 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Sentence[7]: \"De minister kan in Parijs opduiken.\" → [\"Parijs\"/LOC]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('De minister kan in Parijs opduiken.',language_code='nl')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = Classifier.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "# print the sentence with all annotations\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations\n",
    "\n",
    "There are various efforts to combine the outputs of different libraries, \n",
    "particularly where they achieve different things.\n",
    "\n",
    "From what we've seen, many of this seems to be \"let's duct tape it onto spacy\" efforts,\n",
    "which often tries to integrate it into spacy's \"a Document is a series ot Tokens and Spans\" view.\n",
    "\n",
    "Some of these definitely earn the 'duct taped' description.\n",
    "\n",
    "---\n",
    "\n",
    "Others may e.g. wrap another library and presends it as spacy's Document-Token interface,\n",
    "so you can combine and/or compare without the need to rewrite your existing code (e.g. spacy-stanza, spacy-udpipe).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Many of these these can be found in [this project list](https://spacy.io/universe)\n",
    "\n",
    "Note that the examples are _not_ meant as a list of options, just as examples of some added usefulness _and_ of that duct tape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-stanza \n",
    "\n",
    "https://spacy.io/universe/project/spacy-stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain stanza already looks similar:\n",
    "import stanza\n",
    "stanza.download('nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "   \"text\": \"Parijs\",\n",
       "   \"type\": \"LOC\",\n",
       "   \"start_char\": 19,\n",
       "   \"end_char\": 25\n",
       " }]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('nl',logging_level='ERROR')\n",
    "doc = nlp(\"De minister kan in Parijs opduiken.\") # run annotation over a sentence\n",
    "doc.entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 14:03:04.543577: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 14:03:04.793500: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 14:03:07.048329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-01 14:03:07.050960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-01 14:03:07.051086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De de DET det \n",
      "minister minister NOUN nsubj \n",
      "kan kunnen AUX aux \n",
      "in in ADP case \n",
      "Parijs Parijs PROPN obl LOC\n",
      "opduiken op_duiken VERB root \n",
      ". . PUNCT punct \n",
      "(Parijs,)\n"
     ]
    }
   ],
   "source": [
    "# spacy-stanza makes that act like spacy even more\n",
    "import stanza\n",
    "import spacy_stanza\n",
    "\n",
    "stanza.download(\"nl\")\n",
    "nlp = spacy_stanza.load_pipeline(\"nl\")\n",
    "\n",
    "doc = nlp(\"De minister kan in Parijs opduiken.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-benepar, and adding some nltk, why not\n",
    "\n",
    "(Note that benepar does not have a Dutch model, so isn't directly relevant to this project)\n",
    "\n",
    "\n",
    "Spacy has a dependency parser, but not a constituency parser.\n",
    "\n",
    "Constituency parsers give the more classical \"verb phrase containing a noun phrase\" view on sentences. \n",
    "These have fallen out of style somewhat, and there isn't a high quality one for every language,\n",
    "but they definitely still have their uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk, benepar, spacy\n",
    "\n",
    "bpname = 'benepar_en3'\n",
    "benepar.download( bpname )\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(bpname))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\":bpname})\n",
    "\n",
    "doc = nlp(\"I like cheese and you like ham.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"264px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,344.0,264.0\" width=\"344px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"44.186%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"26.3158%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.1579%\" y1=\"20px\" y2=\"48px\" /><svg width=\"73.6842%\" x=\"26.3158%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"42.8571%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">like</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.4286%\" y1=\"20px\" y2=\"48px\" /><svg width=\"57.1429%\" x=\"42.8571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">cheese</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.4286%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.1579%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.093%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.6279%\" x=\"44.186%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"37.2093%\" x=\"55.814%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"31.25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">you</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"68.75%\" x=\"31.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"54.5455%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">like</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.2727%\" y1=\"20px\" y2=\"48px\" /><svg width=\"45.4545%\" x=\"54.5455%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ham</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.2727%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.625%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.4186%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.97674%\" x=\"93.0233%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.5116%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('S', [Tree('NP', [Tree('PRP', ['I'])]), Tree('VP', [Tree('VBP', ['like']), Tree('NP', [Tree('NN', ['cheese'])])])]), Tree('CC', ['and']), Tree('S', [Tree('NP', [Tree('PRP', ['you'])]), Tree('VP', [Tree('VBP', ['like']), Tree('NP', [Tree('NN', ['ham'])])])]), Tree('.', ['.'])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like cheese and /\n",
      "you like ham . /\n"
     ]
    }
   ],
   "source": [
    "for sent in list(doc.sents):\n",
    "    # one random idea based on a constituency parse: to estimate how well sentences at more clause-boundary-like places\n",
    "    # - this basically counts the amount of brackets at each token position (and remembering that in terms of token index)\n",
    "    #   Every time a bunch of brackets are closed at once, we decide we might split there.\n",
    "    # - You _can_ interrogate sent._.constituents for this, and this is arguably a less error-prone thing to do.\n",
    "    #   For the example, though, we take the tree output out, as a string, \n",
    "    #   and then parse that in nltk and do our work there.\n",
    "\n",
    "    ps = sent._.parse_string.replace(\"(\",\"[\").replace(\")\",\"]\") # slightly different markup\n",
    "    nltk_tree = nltk.Tree.fromstring( sent._.parse_string )\n",
    "    display(nltk_tree) # nltk also shows us that parse visually; spacy-benepar doesn't seem to\n",
    "\n",
    "    # will count into these. Defaultdict lets us skip the 'initialize with all indices in the list\n",
    "    start_counts = [0]*(len(sent)+1) # +1 because the below is meant to allow [start:end] so end is one higher than the actual last index\n",
    "    end_counts   = [0]*(len(sent)+1)\n",
    "\n",
    "    for c in sent._.constituents:\n",
    "        start_counts[c.start] += 1 \n",
    "        end_counts[c.end] += 1 \n",
    "\n",
    "    diffs   = list( end_counts[i] - start_counts[i] for i in range(min(len(end_counts), len(start_counts))) )\n",
    "    maxdiff = max(diffs)\n",
    "\n",
    "    for toki, tok in enumerate(sent):\n",
    "        bracket_diff = diffs[toki]\n",
    "        # print the word\n",
    "        print( tok, end=' ' )\n",
    "        # print the amount of closing between this and the next word (basically, the amount that the tree depth decreases)\n",
    "        #print( max(0,bracket_diff), end=' ') # negative means opening, right now we don't care about that so report it as zero\n",
    "\n",
    "        # a bunch of brackets,  or it being a considerable fraction of the depth differences overall (might work better in shallower trees)\n",
    "        if bracket_diff >= 3    or  bracket_diff > 0.4*maxdiff: \n",
    "            print( '/' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-alpino \n",
    "\n",
    "Alpino is a Dutch dependency parser. \n",
    "\n",
    "It looks like spacy's provided dutch models are based on alpino, \n",
    "so you may not care for doing this with extra steps,\n",
    "until you have a specific reason to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy-udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'nl' language\n",
      "De de DET det \n",
      "minister minister NOUN nsubj \n",
      "kan kunnen AUX aux \n",
      "in in ADP case \n",
      "Parijs Parijs PROPN obl \n",
      "opduiken opduiken VERB ROOT \n",
      ". . PUNCT punct \n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import spacy_udpipe\n",
    "spacy_udpipe.download(\"nl\")\n",
    "nlp = spacy_udpipe.load(\"nl\")\n",
    "\n",
    "doc = nlp(\"De minister kan in Parijs opduiken.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
    "print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

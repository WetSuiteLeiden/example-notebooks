{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities\n",
    "Most models add vectors to each token (tok2vec, or transformer-based for the _trf model),\n",
    "which are word embeddings, so should compare well for words with similar meaning.\n",
    "\n",
    "This assists tasks like calculating similarity of larger chunks of text as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "semantic similarity: Compare to best paratgraph\n",
    "so also split better\n",
    "\n",
    "maybe separate repo\n",
    "...with a note that \n",
    "\n",
    "\n",
    "split docs\n",
    "  bwb, cvdr    XML based\n",
    "  rechtspraak  opkayish\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "english_lg  = spacy.load('en_core_web_lg')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869                                      ducks are great                                      cats are nice\n",
      "0.872                                      ducks are great                                     goats are cool\n",
      "0.796                                      ducks are great                                   Forks are spoons\n",
      "0.383                                      ducks are great                                   Forks and spoons\n",
      "0.450                                      ducks are great                        Forks and spoons and knives\n",
      "0.761                                      ducks are great                        Forks and spoons are knives\n",
      "0.868                     ducks and blah and blah and blah                    blue and bleh and bleh and bleh\n",
      "0.218                                                ducks                                               blue\n",
      "\n",
      "0.732\n",
      "  [Because it is smaller, the Moon has less gravity than Earth (only 1/6 of the amount on Earth).]\n",
      "  [So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon.]\n",
      "0.688\n",
      "  [So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon.]\n",
      "  [But even though the Moon's gravity is weaker than the Earth's gravity, it is still there.]\n",
      "0.764\n",
      "  [But even though the Moon's gravity is weaker than the Earth's gravity, it is still there.]\n",
      "  [If a person dropped a ball while standing on the moon, it would still fall down.]\n",
      "0.803\n",
      "  [If a person dropped a ball while standing on the moon, it would still fall down.]\n",
      "  [However, it would fall much more slowly.]\n",
      "0.763\n",
      "  [However, it would fall much more slowly.]\n",
      "  [A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.]\n",
      "0.815\n",
      "  [A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.]\n",
      "  [Rome ceased to be the capital from the time of the division.]\n",
      "0.687\n",
      "  [Rome ceased to be the capital from the time of the division.]\n",
      "  [In 286, the capital of the Western Roman Empire became Mediolanum (now Milan).]\n",
      "0.737\n",
      "  [In 286, the capital of the Western Roman Empire became Mediolanum (now Milan).]\n",
      "  [In 402, the capital was again moved, this time to Ravenna.]\n",
      "0.911\n",
      "  [In 402, the capital was again moved, this time to Ravenna.]\n",
      "  [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "0.743\n",
      "  [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "  [By 410, he had sacked the Rome.]\n",
      "0.752\n",
      "  [By 410, he had sacked the Rome.]\n",
      "  [In 455, the Vandals captured the city.]\n",
      "0.973\n",
      "  [In 455, the Vandals captured the city.]\n",
      "  [In 476, the Goths captured the capital]\n"
     ]
    }
   ],
   "source": [
    "# You can generally compare any token/span (spans/sentence/docs will act as their average) using .similarity(). \n",
    "# \n",
    "# There are some fundamental limitations to this to keep in mind, like \n",
    "# - that it does not consider ordering, just words' presence\n",
    "# - how volatile the meaning of short sentences may be\n",
    "# - how function words dilute larger-span vectors (and might make them compare well for non-contentful reasons)\n",
    "# - 'static vectors' basically means a word has the same vector in all contexts.\n",
    "\n",
    "for one, other in (\n",
    "        ('ducks are great', 'cats are nice'),\n",
    "        ('ducks are great', 'goats are cool'),\n",
    "        ('ducks are great', 'Forks are spoons'),\n",
    "        ('ducks are great', 'Forks and spoons'),\n",
    "        ('ducks are great', 'Forks and spoons and knives'),\n",
    "        ('ducks are great', 'Forks and spoons are knives'),\n",
    "        ('ducks and blah and blah and blah', 'blue and bleh and bleh and bleh'),\n",
    "        ('ducks',           'blue'),\n",
    "    ):\n",
    "    sim = english_lg( one ).similarity(english_lg( other ))\n",
    "    print( \"%.3f   %50s %50s\"%( sim, one, other ))\n",
    "\n",
    "# These three-world phrases are actually quite contrived - real sentences have a narrower range of \n",
    "\n",
    "print()\n",
    "# these are sentences from two different wikipedia articles; we are trying to see if it can tell a difference.\n",
    "text = \"\"\"Because it is smaller, the Moon has less gravity than Earth (only 1/6 of the amount on Earth). \n",
    "So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon. \n",
    "But even though the Moon's gravity is weaker than the Earth's gravity, it is still there. \n",
    "If a person dropped a ball while standing on the moon, it would still fall down. However, it would fall much more slowly.\n",
    "A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.\n",
    "Rome ceased to be the capital from the time of the division. \n",
    "In 286, the capital of the Western Roman Empire became Mediolanum (now Milan). \n",
    "In 402, the capital was again moved, this time to Ravenna.\n",
    "In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.\n",
    "By 410, he had sacked the Rome. In 455, the Vandals captured the city. In 476, the Goths captured the capital \"\"\"\n",
    "\n",
    "doc = english_lg( text )\n",
    "sents = list(doc.sents)\n",
    "for i in range(len(sents)-1):\n",
    "    one, other = sents[i], sents[i+1] \n",
    "    sim = one.similarity( other )\n",
    "    print( \"%.3f\\n  [%s]\\n  [%s]\"%( sim, one.text.strip(), other.text.strip() ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spacy has, however, made things a bit complex.\n",
    "- _sm models don't have vectors.  While similarity() still does something useful at all, you should assume this is extremely basic.\n",
    "- _md and _lg  models tend to have **static word vectors**, for english it may be GroVe vectors, great in itself _but_ a word will receive the same vector in all contexts\n",
    "- _trf do context sensitive embeddings (and put those values in a different place)\n",
    "\n",
    "This means that\n",
    "* spacy's similarity() call does different things in difference models\n",
    "* you can't always pick out the vectors directly (though you can often get away with it if you stick to one model)\n",
    "\n",
    "\n",
    "Note that\n",
    "- scores on spans and docs act as the average of their compobnents\n",
    "- ...which also means e.g. function words can dilute larger-span vectors (and might make them compare well for non-contentful reasons)\n",
    "- (...so...) similarity() does not consider ordering, just words' presence\n",
    "- shorter sentences have minimal and more volative meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!python3 -m spacy download en_core_web_trf   # works better, but can be rather slow without GPU properly set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "english_trf = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "# We use the @ character to register the following Class definition\n",
    "# with spaCy under the name 'tensor2attr'.\n",
    "@Language.factory('tensor2attr')\n",
    "# We begin by declaring the class name: Tensor2Attr. The name is\n",
    "# declared using 'class', followed by the name and a colon.\n",
    "class Tensor2Attr:\n",
    "\n",
    "    # We continue by defining the first method of the class,\n",
    "    # __init__(), which is called when this class is used for\n",
    "    # creating a Python object. Custom components in spaCy\n",
    "    # require passing two variables to the __init__() method:\n",
    "    # 'name' and 'nlp'. The variable 'self' refers to any\n",
    "    # object created using this class!\n",
    "    def __init__(self, name, nlp):\n",
    "        # We do not really do anything with this class, so we\n",
    "        # simply move on using 'pass' when the object is created.\n",
    "        pass\n",
    "\n",
    "    # The __call__() method is called whenever some other object\n",
    "    # is passed to an object representing this class. Since we know\n",
    "    # that the class is a part of the spaCy pipeline, we already know\n",
    "    # that it will receive Doc objects from the preceding layers.\n",
    "    # We use the variable 'doc' to refer to any object received.\n",
    "    def __call__(self, doc):\n",
    "        # When an object is received, the class will instantly pass\n",
    "        # the object forward to the 'add_attributes' method. The\n",
    "        # reference to self informs Python that the method belongs\n",
    "        # to this class.\n",
    "        self.add_attributes(doc)\n",
    "\n",
    "        # After the 'add_attributes' method finishes, the __call__\n",
    "        # method returns the object.\n",
    "        return doc\n",
    "\n",
    "    # Next, we define the 'add_attributes' method that will modify\n",
    "    # the incoming Doc object by calling a series of methods.\n",
    "    def add_attributes(self, doc):\n",
    "        # spaCy Doc objects have an attribute named 'user_hooks',\n",
    "        # which allows customising the default attributes of a\n",
    "        # Doc object, such as 'vector'. We use the 'user_hooks'\n",
    "        # attribute to replace the attribute 'vector' with the\n",
    "        # Transformer output, which is retrieved using the\n",
    "        # 'doc_tensor' method defined below.\n",
    "        doc.user_hooks['vector'] = self.doc_tensor\n",
    "\n",
    "        # We then perform the same for both Spans and Tokens that\n",
    "        # are contained within the Doc object.\n",
    "        doc.user_span_hooks['vector'] = self.span_tensor\n",
    "        doc.user_token_hooks['vector'] = self.token_tensor\n",
    "\n",
    "        # We also replace the 'similarity' method, because the\n",
    "        # default 'similarity' method looks at the default 'vector'\n",
    "        # attribute, which is empty! We must first replace the\n",
    "        # vectors using the 'user_hooks' attribute.\n",
    "        doc.user_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_span_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_token_hooks['similarity'] = self.get_similarity\n",
    "\n",
    "    # Define a method that takes a Doc object as input and returns\n",
    "    # Transformer output for the entire Doc.\n",
    "    def doc_tensor(self, doc):\n",
    "        # Return Transformer output for the entire Doc. As noted\n",
    "        # above, this is the last item under the attribute 'tensor'.\n",
    "        # Average the output along axis 0 to handle batched outputs.\n",
    "        return doc._.trf_data.tensors[-1].mean(axis=0)\n",
    "\n",
    "    # Define a method that takes a Span as input and returns the Transformer\n",
    "    # output.\n",
    "    def span_tensor(self, span):\n",
    "        # Get alignment information for Span. This is achieved by using\n",
    "        # the 'doc' attribute of Span that refers to the Doc that contains\n",
    "        # this Span. We then use the 'start' and 'end' attributes of a Span\n",
    "        # to retrieve the alignment information. Finally, we flatten the\n",
    "        # resulting array to use it for indexing.\n",
    "        tensor_ix = span.doc._.trf_data.align[span.start: span.end].data.flatten()\n",
    "\n",
    "        # Fetch Transformer output shape from the final dimension of the output.\n",
    "        # We do this here to maintain compatibility with different Transformers,\n",
    "        # which may output tensors of different shape.\n",
    "        out_dim = span.doc._.trf_data.tensors[0].shape[-1]\n",
    "\n",
    "        # Get Token tensors under tensors[0]. Reshape batched outputs so that\n",
    "        # each \"row\" in the matrix corresponds to a single token. This is needed\n",
    "        # for matching alignment information under 'tensor_ix' to the Transformer\n",
    "        # output.\n",
    "        tensor = span.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "\n",
    "        # Average vectors along axis 0 (\"columns\"). This yields a 768-dimensional\n",
    "        # vector for each spaCy Span.\n",
    "        return tensor.mean(axis=0)\n",
    "\n",
    "    # Define a function that takes a Token as input and returns the Transformer\n",
    "    # output.\n",
    "    def token_tensor(self, token):\n",
    "        # Get alignment information for Token; flatten array for indexing.\n",
    "        # Again, we use the 'doc' attribute of a Token to get the parent Doc,\n",
    "        # which contains the Transformer output.\n",
    "        tensor_ix = token.doc._.trf_data.align[token.i].data.flatten()\n",
    "\n",
    "        # Fetch Transformer output shape from the final dimension of the output.\n",
    "        # We do this here to maintain compatibility with different Transformers,\n",
    "        # which may output tensors of different shape.\n",
    "        out_dim = token.doc._.trf_data.tensors[0].shape[-1]\n",
    "\n",
    "        # Get Token tensors under tensors[0]. Reshape batched outputs so that\n",
    "        # each \"row\" in the matrix corresponds to a single token. This is needed\n",
    "        # for matching alignment information under 'tensor_ix' to the Transformer\n",
    "        # output.\n",
    "        tensor = token.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "\n",
    "        # Average vectors along axis 0 (columns). This yields a 768-dimensional\n",
    "        # vector for each spaCy Token.\n",
    "        return tensor.mean(axis=0)\n",
    "\n",
    "    # Define a function for calculating cosine similarity between vectors\n",
    "    def get_similarity(self, doc1, doc2):\n",
    "        # Calculate and return cosine similarity\n",
    "        return np.dot(doc1.vector, doc2.vector) / (doc1.vector_norm * doc2.vector_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding transformer based similarity\n"
     ]
    }
   ],
   "source": [
    "# while _trf can do contextual word embedding rather than static word embedding, this is not placed in .vector.tensors\n",
    "# You could fish out the tensors like   toks_vectors, doc_vector = doc._.trf_data.tensors\n",
    "#   but it's handier to augent spacy to make similarity work with transformer tensors - custom pipeline element defined in our helper module\n",
    "\n",
    "# this mentioned tensor2attr is not basic spacy, it exists because it is defined in our helpers_spacy\n",
    "if not english_trf.has_pipe('tensor2attr'):\n",
    "    print(\"adding transformer based similarity\")\n",
    "    english_trf.add_pipe('tensor2attr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.tokens\n",
    "spacy.tokens.Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital capital 0.61\n"
     ]
    }
   ],
   "source": [
    "doc = english_trf(\"The bank stores investment capital in Paris, the capital of France\")\n",
    "first_capital, second_capital = doc[4], doc[9]\n",
    "print( first_capital, second_capital, round( first_capital.similarity(second_capital), 2) ) \n",
    "# without contextual word embeddings, both capitals would be the same, and their similarity 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               money city lyon\n",
      "  bank_capital  0.55 0.48 0.32\n",
      "   city_capital 0.35 0.34 0.15\n"
     ]
    }
   ],
   "source": [
    "# figure out what word it's close to \n",
    "# TODO: fix that this is less valid because we are comparting to words that are ripped from context\n",
    "money  = english_trf(\"money\")[0]\n",
    "city   = english_trf(\"city\")[0]\n",
    "paris  = english_trf(\"lyon\")[0]\n",
    "\n",
    "print ('              ','money', 'city', 'lyon')\n",
    "for in_sent, example in ( (' bank_capital ',first_capital), ('city_capital',second_capital) ):\n",
    "    print( '%15s %4.2f %4.2f %4.2f'%(in_sent, round(example.similarity(money), 2), round( example.similarity(city), 2), round( example.similarity(paris), 2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "#vocablist = list(english_lg.vocab.strings)\n",
    "vvv = []\n",
    "i=0\n",
    "for string in list(english_lg.vocab.strings)[::10]:\n",
    "    if len(string)<4:\n",
    "        continue\n",
    "    n = english_lg(string)[0].norm\n",
    "    if n > 800000000000000000:\n",
    "        continue\n",
    "\n",
    "    print( n, string, english_lg.vocab.strings[string])\n",
    "    \n",
    "    if string in english_lg.vocab.vectors:\n",
    "        print( string, english_lg.vocab.strings[string], numpy.abs(english_lg.vocab.vectors[string]))\n",
    "    \n",
    "    vvv.append( string )\n",
    "\n",
    "    i+=1\n",
    "    if i>1000:\n",
    "        break\n",
    "print(i)\n",
    "print(len(vvv))\n",
    "\n",
    "vocablist = vvv\n",
    "\n",
    "#english_lg.vocab.lookups\n",
    "#for string in \n",
    "#print(len(vocablist))\n",
    "#vocablist[0].prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocablist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m allsim1, allsim2 \u001b[38;5;241m=\u001b[39m {}, {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#vocablist = list(english_lg.vocab.strings)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;28mlen\u001b[39m(\u001b[43mvocablist\u001b[49m) )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocablist[::\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     isolated \u001b[38;5;241m=\u001b[39m english_trf(word)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocablist' is not defined"
     ]
    }
   ],
   "source": [
    "#english_md = spacy.load(\"en_core_web_md\")   \n",
    "\n",
    "allsim1, allsim2 = {}, {}\n",
    "#vocablist = list(english_lg.vocab.strings)\n",
    "\n",
    "print( len(vocablist) )\n",
    "\n",
    "for word in vocablist[::10]:\n",
    "    isolated = english_trf(word)\n",
    "    allsim1[word] = first_capital.similarity( isolated )\n",
    "    allsim2[word] = second_capital.similarity( isolated )\n",
    "\n",
    "allsim1 = list(allsim1.items())\n",
    "allsim1.sort(key = lambda x:x[1], reverse=True)\n",
    "print( allsim1[:10] )\n",
    "\n",
    "allsim2 = list(allsim2.items())\n",
    "allsim2.sort(key = lambda x:x[1], reverse=True)\n",
    "print( allsim2[:10] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things like 'find similar words within texts' will rely on some variant of 'compare everything to everything'\n",
    "# I've not found a spacy way to do such mass comparisons other than to call .similarity() a lot, which is a bunch of overhead\n",
    "# Since it seems to just be cosine similarity, we can use scipy to do a lot more comparisons in one go - code for which is in our helper\n",
    "\n",
    "print( \"SENTENCE SIMILARITY\" )\n",
    "# yes, these these thresholds are chosen to give good results with this example. Play with them to see how messy it actually is.\n",
    "for score, one, two in wetsuite.helpers.spacy.similar_sentences(doc,     thresh=0.5, n=5):   \n",
    "    print( \"    %5.2f  %40r  %40r\"%(score, one, two) )\n",
    "    \n",
    "print( \"TOKEN SIMILARITY\" )\n",
    "for score, one, two in wetsuite.helpers.spacy.similar_chunks(doc, 1,0,0, thresh=0.6, n=5):\n",
    "    print( \"    %5.2f  %40s  %40s\"%(score, one, two) )\n",
    "\n",
    "print( \"ENTITY AND NOUN CHUNK SIMILARITY\" )\n",
    "for score, one, two in wetsuite.helpers.spacy.similar_chunks(doc, 0,1,1, thresh=0.7, n=5):\n",
    "    print( \"    %5.2f  %40s  %40s\"%(score, one, two) )\n",
    "\n",
    "# It's generally not so useful to compare tokens with phrases from the same document, in that the top similarities will be phrases with their own head/root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the average of a sentence or document would be a lot of function words, \n",
    "#   direct comparison would still work but be watered down depending on how many of those there are\n",
    "\n",
    "\n",
    "#   so you might like \n",
    "# At the same time, spacy prefers its parsed object immutable, so you would have to work around it\n",
    "import numpy\n",
    "from importlib import reload\n",
    "reload(helpers_spacy)\n",
    "for sent in paris.sents:\n",
    "    print( '-'*80 )\n",
    "    print( sent )\n",
    "    sg = helpers_spacy.interesting_words( sent )\n",
    "    print( sg )\n",
    "    vpt = helpers_spacy.vector_per_tag(sent, average=True) \n",
    "    for tag, ary in vpt.items():\n",
    "        print( tag, numpy.linalg.norm(ary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

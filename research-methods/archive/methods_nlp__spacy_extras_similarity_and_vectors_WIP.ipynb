{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities\n",
    "\n",
    "Before we talk about _how_ things are made comparable, let's demonstate _that_ they are.\n",
    "\n",
    "You can generally use .similarity() to compare any token --\n",
    "as well as larger units like span or doc, as they will mean the average of their parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869                                      ducks are great                                      cats are nice\n",
      "0.872                                      ducks are great                                     goats are cool\n",
      "0.757                                      ducks are great                                   Forks are spoons\n",
      "0.307                                      ducks are great                                   Forks and spoons\n",
      "0.420                                      ducks are great                        Forks and spoons and knives\n",
      "0.750                                      ducks are great                        Forks and spoons are knives\n",
      "0.858                     ducks and blah and blah and blah                    blue and bleh and bleh and bleh\n",
      "0.533                                                ducks                                               cats\n",
      "0.218                                                ducks                                               blue\n",
      "0.811                                                  red                                               blue\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "english_md  = spacy.load('en_core_web_md')   \n",
    "\n",
    "for one, other in ( # compare some prepared pairs\n",
    "        ('ducks are great', 'cats are nice'),\n",
    "        ('ducks are great', 'goats are cool'),\n",
    "        ('ducks are great', 'Forks are spoons'),\n",
    "        ('ducks are great', 'Forks and spoons'),\n",
    "        ('ducks are great', 'Forks and spoons and knives'),\n",
    "        ('ducks are great', 'Forks and spoons are knives'),\n",
    "        ('ducks and blah and blah and blah', 'blue and bleh and bleh and bleh'),\n",
    "        ('ducks',           'cats'),\n",
    "        ('ducks',           'blue'),\n",
    "        ('red',             'blue'),\n",
    "    ):\n",
    "    sim = english_md( one ).similarity(english_md( other ))\n",
    "    print( \"%.3f   %50s %50s\"%( sim, one, other ))\n",
    "\n",
    "# These three-world phrases are actually quite contrived - real sentences have a narrower range of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some fundamental limitations to this to keep in mind, like \n",
    "- assume it does not consider ordering, just words' presence\n",
    "- the shorter the sentence/sequence, the more that one word of difference changes its meaning\n",
    "- the longer the sequence, the less that an average of meaning means much\n",
    "- at any length, how function words will dilute meaning (and might make them compare well to others for _non-contentful_ reasons)\n",
    "- how similar filler words can make two sentences more alike than they really are \n",
    "- 'static vectors' basically means a word has the same vector in all contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "       [In 476, the Goths captured the capital.]\n",
      "       [In 455, the Vandals captured the city.]\n",
      "0.91\n",
      "       [In 402, the capital was again moved, this time to Ravenna.]\n",
      "       [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "0.91\n",
      "       [In 476, the Goths captured the capital.]\n",
      "       [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "0.91\n",
      "       [In 402, the capital was again moved, this time to Ravenna.]\n",
      "       [In 476, the Goths captured the capital.]\n",
      "0.90\n",
      "       [In 402, the capital was again moved, this time to Ravenna.]\n",
      "       [In 455, the Vandals captured the city.]\n",
      "0.90\n",
      "       [In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.]\n",
      "       [In 455, the Vandals captured the city.]\n",
      "0.88\n",
      "       [In 286, the capital of the Western Roman Empire became Mediolanum (now Milan).]\n",
      "       [In 476, the Goths captured the capital.]\n",
      "0.88\n",
      "       [If a person dropped a ball while standing on the moon, it would still fall down.]\n",
      "       [A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.]\n",
      "0.86\n",
      "       [In 286, the capital of the Western Roman Empire became Mediolanum (now Milan).]\n",
      "       [In 455, the Vandals captured the city.]\n",
      "0.86\n",
      "       [So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon.]\n",
      "       [If a person dropped a ball while standing on the moon, it would still fall down.]\n"
     ]
    }
   ],
   "source": [
    "# these are sentences from two different wikipedia articles; we are trying to see if it can tell a difference.\n",
    "text = \"\"\"\n",
    "Because it is smaller, the Moon has less gravity than Earth (only 1/6 of the amount on Earth). \n",
    "\n",
    "In 286, the capital of the Western Roman Empire became Mediolanum (now Milan). \n",
    "\n",
    "So if a person weighs 60 kilograms on Earth, the person would only weigh 10 kilograms on the moon. \n",
    "\n",
    "In 402, the capital was again moved, this time to Ravenna.\n",
    "\n",
    "But even though the Moon's gravity is weaker than the Earth's gravity, it is still there. \n",
    "\n",
    "In 476, the Goths captured the capital.\n",
    "\n",
    "If a person dropped a ball while standing on the moon, it would still fall down. However, it would fall much more slowly.\n",
    "\n",
    "In AD 398, Alaric led the Visigoths and began making attacks closer and closer to the capital.\n",
    "\n",
    "A person who jumped as high as possible on the moon would jump higher than on Earth, but still fall back to the ground.\n",
    "\n",
    "Its surface gravity is about one sixth of Earth's, about half of that of Mars.\n",
    "\n",
    "Rome ceased to be the capital from the time of the division. \n",
    "\n",
    "The Moon has been an important source of inspiration and knowledge for humans.\n",
    "\n",
    "In 455, the Vandals captured the city. \n",
    "\"\"\"\n",
    "\n",
    "# use the model to split into sentences.\n",
    "doc = english_md( text )\n",
    "sents = list(doc.sents) \n",
    "# and see which sentences match best\n",
    "res = []\n",
    "for i in range(len(sents)-1):\n",
    "    for j in range(i+1, len(sents)):\n",
    "        one, other = sents[i], sents[j] \n",
    "        sim = one.similarity( other )\n",
    "        if sim > 0.85:\n",
    "            res.append( (sim, one, other) )\n",
    "res.sort(reverse=True)\n",
    "\n",
    "for sim, one, other in res:\n",
    "     print( \"%.2f\\n       [%s]\\n       [%s]\"%( sim, one.text.strip(), other.text.strip() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that looks good, but such examples are often a little... synthetic. \n",
    "\n",
    "Yes, it puts things that are alike together even though they separated in the text,\n",
    "but it's not clear what the reason for the difference between 0.9 and 0.7 scores are,\n",
    "and below 0.7 it starts to make mistakes even with these fairly distinct topics.\n",
    "\n",
    "We hid that fact from you by having a higher threshold.\n",
    "\n",
    "Also, as spacy documentation points out..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity is subjective\n",
    "\n",
    "Say, in the context of any text whatsoever, `I like burgers` and `I like pasta` are quite similar,\n",
    "in the context of food, they should be considered quite dissimilar ([from here](https://spacy.io/usage/spacy-101#vectors-similarity))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's also technically messy\n",
    "\n",
    "We can assume these are 'word embeddings', which often just means 'vectors, but we spent a bunch of time compacting meaning into relatively few dense dimensions'.\n",
    "\n",
    "\n",
    "similarity() is nice, but you may have seen other uses of this semantic sort of thing.\n",
    "\n",
    "You may have seen:\n",
    "- semantic comparison\n",
    "  - \"how similar are two documents, by what they talk about?\"\n",
    "\n",
    "- semantic search \n",
    "  - querying for documents and allowing for variations in the words used to express ideas -- by querying by such meaningful vectors\n",
    "\n",
    "\n",
    "For many of these, you will \n",
    "- possible train it for your use\n",
    "- need to fetch the underlying numbers\n",
    "- ensure the remain the same between all things you compare\n",
    "\n",
    "All of that takes a bunch more planning, forethought, and design up front.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also takes insight into spacy in particular works.\n",
    "\n",
    "The thing that makes it very flexible and modular _also_ means that not all vectors are the same, _at all_.\n",
    "\n",
    "If you squint, vectors could be said to come from \n",
    "- static vectors  - come from predetermined, always the same for the same word\n",
    "- dynamic vectors - using context in the sentence it is used\n",
    "\n",
    "Spacy seems to have all combinations\n",
    "- neither - models without any will exist, though do not seem common.\n",
    "- static only - \n",
    "- static and dynamic - start with static vectors, then adjust for some contextual awareness, e.g. when detecting something as a VERB, adjust for verbiness\n",
    "- dynamic only - \n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "Models may\n",
    "- use static vectors only\n",
    "\n",
    "  - e.g. with tok2vec\n",
    "- and the tiny models are even context-sensitive-only which, yes, is confusing.\n",
    "- transformers work in their own way\n",
    "\n",
    "As a result, the same text may get different vectors.\n",
    "\n",
    "\n",
    "\n",
    "There is a general machine-learning question in how do you deal with new data.\n",
    "\n",
    "One way is to avoid needing to at all. \n",
    "Say, use one model to analyse all text, and stick with those vectors.\n",
    "\n",
    "Another is to \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Spacy has, however, made things a bit complex.\n",
    "- _sm models don't have vectors.  While similarity() still does something at all, you should assume this is extremely basic.\n",
    "- _md and _lg  models tend to have **static word vectors**, for english it may be [https://nlp.stanford.edu/projects/glove/ GLoVe vectors], great in itself _but_ a word will receive the same vector in all contexts\n",
    "- _trf do context sensitive embeddings (and put those values in a different place)\n",
    "\n",
    "This means that\n",
    "* spacy's similarity() call does different things in differenct models\n",
    "* you can't always pick out the vectors directly (though you can often get away with it if you stick to one model)\n",
    "\n",
    "\n",
    "Note that\n",
    "- scores on spans and docs act as the average of their compobnents\n",
    "- ...which also means e.g. function words can dilute larger-span vectors (and might make them compare well for non-contentful reasons)\n",
    "- (...so...) similarity() does not consider ordering, just words' presence\n",
    "- shorter sentences have minimal and more volative meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wetsuite.helpers.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things like 'find similar words within texts' will rely on some variant of 'compare everything to everything'\n",
    "# I've not found a spacy way to do such mass comparisons other than to call .similarity() a lot, which is a bunch of overhead\n",
    "# Since it seems to just be cosine similarity, we can use scipy to do a lot more comparisons in one go - code for which is in our helper\n",
    "\n",
    "print( \"SENTENCE SIMILARITY\" )\n",
    "# yes, these these thresholds are chosen to give good results with this example. Play with them to see how messy it actually is.\n",
    "for score, one, two in wetsuite.helpers.spacy.similar_sentences(doc,     thresh=0.5, n=5):   \n",
    "    print( \"    %5.2f  %40r  %40r\"%(score, one, two) )\n",
    "    \n",
    "print( \"TOKEN SIMILARITY\" )\n",
    "for score, one, two in wetsuite.helpers.spacy.similar_chunks(doc, 1,0,0, thresh=0.6, n=5):\n",
    "    print( \"    %5.2f  %40s  %40s\"%(score, one, two) )\n",
    "\n",
    "print( \"ENTITY AND NOUN CHUNK SIMILARITY\" )\n",
    "for score, one, two in wetsuite.helpers.spacy.similar_chunks(doc, 0,1,1, thresh=0.7, n=5):\n",
    "    print( \"    %5.2f  %40s  %40s\"%(score, one, two) )\n",
    "\n",
    "# It's generally not so useful to compare tokens with phrases from the same document, in that the top similarities will be phrases with their own head/root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the average of a sentence or document would be a lot of function words, \n",
    "#   direct comparison would still work but be watered down depending on how many of those there are\n",
    "\n",
    "\n",
    "#   so you might like \n",
    "# At the same time, spacy prefers its parsed object immutable, so you would have to work around it\n",
    "import numpy\n",
    "from importlib import reload\n",
    "reload(helpers_spacy)\n",
    "for sent in paris.sents:\n",
    "    print( '-'*80 )\n",
    "    print( sent )\n",
    "    sg = helpers_spacy.interesting_words( sent )\n",
    "    print( sg )\n",
    "    vpt = helpers_spacy.vector_per_tag(sent, average=True) \n",
    "    for tag, ary in vpt.items():\n",
    "        print( tag, numpy.linalg.norm(ary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the simpler of word embeddings / vectors\n",
    "This assists tasks like calculating similarity of larger chunks of text as well.\n",
    "\n",
    "\n",
    "\n",
    "semantic similarity: Compare to best paratgraph\n",
    "so also split better\n",
    "\n",
    "maybe separate repo\n",
    "...with a note that \n",
    "\n",
    "\n",
    "split docs\n",
    "  bwb, cvdr    XML based\n",
    "  rechtspraak  opkayish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

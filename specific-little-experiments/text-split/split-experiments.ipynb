{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3ztv-SU0uO"
      },
      "source": [
        "If you want to start playing with this without installation, try: &nbsp; \n",
        "<a href=\"https://colab.research.google.com/github/knobs-dials/wetsuite-split/blob/main/split-experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (only) in colab, run this first to install wetsuite from (the most recent) source.   For your own setup, see wetsuite's install guidelines.\n",
        "#!pip3 install -U --no-cache-dir --quiet https://github.com/knobs-dials/wetsuite-dev/archive/refs/heads/main.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDSic6hRU0uP"
      },
      "source": [
        "# Purpose of this notebook\n",
        "\n",
        "Figure out how to open various formats of documents, and split them into smaller pieces of text.\n",
        "\n",
        "Probably at a paragraph-like level, and providing document-structural hints where we can -- \n",
        "while explaining why we can _not_ go as far as guaranteeing anything like te natural structure of a document.\n",
        "\n",
        "\n",
        "Note: this notebook is more about the _development_ of this idea, discussion of the tradeoffs, and such. \n",
        "There will be a simpler notebook in wetsuite-notebooks later that gives some examples as to its _use_ on existing datasets\n",
        "(and the sources they come from).\n",
        "\n",
        "This entire notebook may disappear into a short summary later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting, but also splitting\n",
        "\n",
        "It is probably useful if this project tries to parse each kind of document that it itself provides,\n",
        "as well as other kinds you are likely to use.\n",
        "\n",
        "At least to flatten varied formats into plain text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and there are also varied methods that would benefit from receiving such text into bite-sized chunks,\n",
        "and/or for those to be split in a way that is at least _somewhat_ useful.\n",
        "\n",
        "You might e.g.\n",
        "- try to ignore introductory things like \"Wij Beatrix, bij de gratie Gods\" thing in laws,\n",
        "  - because there's a bunch of named enitities there, and few to none of them are even relevant to the law.\n",
        "  - seeing that within the text, and knowing we have a chunk of text no larger than a paragraph, makes it easy to ignore.\n",
        "\n",
        "- up to trying to e.g. find similar court decisions based on some content analysis \n",
        "  e.g.\n",
        "   - focus on the introduction to extract topic, focus on decision to estimate what was done with that topic,\n",
        "   - possibly ignoring discussion/argumentation in the middle\n",
        "   - possibly ignoring the definitions (can be a good indicator, but only useful in comparisons if both documents have it)\n",
        "\n",
        "- split text whenever it seems to be switching to different topic \n",
        "\n",
        "  - e.g. by tring to figure out what section each paragraph belongs to \n",
        "    (whether hinted at by the document or even analysed)\n",
        "\n",
        "- try tricks like feeding smaller chunks at a time into a \"nearby words\" type method\n",
        "  to see which relations come out most _consistently_ throughout a document.\n",
        "  - (you may then care more about the amount of distance and less about the split being natural)\n",
        "\n",
        "...meaning an automated way to suggests which parts to ignore and/or what parts are,\n",
        "even if crude, could be a start. \n",
        "\n",
        "If we can, we want to support such approaches, at least a little."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why we can't go very far in this\n",
        "\n",
        "Varying goals, and varying input.\n",
        "\n",
        "\n",
        "On the varying input side, consider that you as use expect something sensible for _every_ document.\n",
        "\n",
        "Maybe \n",
        "you can give very well-labeled sections, citations, vocabulary references for one document type.\n",
        "Or it's more document-like but consistent enough that once you are used to it, you _can_ pick out a lot of structure.\n",
        "...but if you mix it with PDFs you may not even be able to guaranee unbroken sentences.\n",
        "\n",
        "\n",
        "So we would want to provide something that works similarly well for all input,\n",
        "but the more varied the input is, the more that such a common denominator is... not a lot.\n",
        "\n",
        "Yes, we can try to invent some overlap, or intended structure, and wrangle each format into that,\n",
        "and there is probably some value, \n",
        "yet that kind of creativity, if present at all,\n",
        "should be tools in your hands as a researcher,\n",
        "should be verifiable and not just trusted,\n",
        "not transparently and quietly decided for you.\n",
        "\n",
        "\n",
        "The problen here is that depending on what you do, you may get a mix of great and terrible,\n",
        "a mix that you did not expect, and that is almost as hard to understand as doing it yourself.\n",
        "\n",
        "\n",
        "\n",
        "On the goals side, it depends a little on who our userbase is.\n",
        "* Legal researchers will often find one topic, one data source\n",
        "  - The abovementioned mix issue isn't necessarily a problem, in that documents should be fairly consistent within that\n",
        "\n",
        "* NLP researchers, on the other hand, will probably just take the \"more text is better approach\"\n",
        "  - The abovementioned may or may not be an issue, because they may care _only_ about having a lot of text,\n",
        "    and doing any labeling themselves\n",
        "\n",
        "You can often do somehwat better once you focus on a specific kind of document, source, reseach topic, and such.\n",
        "\n",
        "\n",
        "\n",
        "So this notebook is tools and examples for, also to help instill how well, or how poorly, it works on the documents you work on.\n",
        "\n",
        "Even _the way_ it currently works should be considered just one preliminary way of _maybe_ doing it -- you should treat it as provisionary,\n",
        "as a \"I'll take out all the bits I need and do it myself (and in the cases it happens to already do what I want, great)\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## On an implementation level\n",
        "\n",
        "How we envision this is:\n",
        " * in part about getting text out of different formats - HTML, XML, PDF, possibly document formats\n",
        " * in part about getting small fragments of text out of each, with some supporting information\n",
        " * and let _you_ decide when to join or split those fragments, based on the added information.\n",
        "\n",
        "We would probably end up with a list of handlers like\n",
        " - if you say you recognize this format\n",
        " - read it yourself and hand out the parts\n",
        " - suggest how to split it\n",
        "\n",
        "\n",
        "The \"I recognize this\" should probably have a specificity, e.g. \n",
        " - I know this is a PDF, from Officiele Publicaties, and a specific waterschappen-specific template\n",
        " - I know this is a PDF, from Officiele Publicaties\n",
        " - I know this is a PDF, I'll give you ''something''\n",
        "\n",
        "One reason for this approach is that this can be extended and refined over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This still begs a number of questions\n",
        "- how to do that with different input formats?\n",
        "  - beyond the \"detect type how?\" and \"open file how?\" level, also...\n",
        "\n",
        "- how granular should the output be? Sections? Paragraphs? Sentences? Blocks that might actually be split at arbitrary points belong together?\n",
        "  - it's probably easier to join later than to split later, so smallish is good. Paragraphs?\n",
        "\n",
        "- can we provide intermediate data that leaves some decisions up to you? It would e.g. be nice if you could use the same  thing to \n",
        "  - get sections\n",
        "  - get paragraphs\n",
        "  - break up the thing to get e.g. ~200 words at a time, almost regardless of structure\n",
        "\n",
        "\n",
        "- how much are users expected to do, how much smartness could be merged in later?\n",
        "  - e.g. \"hey this header says 1.  and the next one says 2.\" needs some refinement but can later be quite useful\n",
        "  - and does that imply that the meta could also use hints like \"hint:[ ('feature', 'new-section'), (probability, 0.7)}\"\n",
        "\n",
        "- can we have any document-type-specific handling? (e.g. remove headers and footers from PDF)\n",
        "\n",
        "- can we have any document-set-specific handling? (e.g. \"kamervraag PDFs use a neat template, we can ease _just_ the text out of it fairly easily\")\n",
        "  - and have that be extensible, so we can incrementally improve it?\n",
        "\n",
        "- how much of that is for future prohjects because it really is to omuch now?\n",
        "\n",
        "- how useful is it to point back to the original?\n",
        "  - in XML laws this might be useful. In most others not so much. This might be out of scope, really.\n",
        "\n",
        "\n",
        "Indeed, to provide a relatively universal intermediate document format is a nontrivial exercise even when you care only\n",
        "about the _aesthetics_ of the result, let alone when seeming to make any promises about the structure you hand over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some experiments\n",
        "\n",
        "TODO\n",
        "<!--\n",
        "\n",
        "is a scope of 'how good/bad is it to split here',\n",
        "which you can force into smaller and larger chunks via some parameters.\n",
        "\n",
        "\n",
        "What you can expect\n",
        "- a stream of (metadata, text)\n",
        "\n",
        "- a function that handles handles that at somewhat higher level, like\n",
        "  - get section-sized things\n",
        "  - get paragraph-sized fragments\n",
        "  - break up the thing to get e.g. ~200 words at a time, almost regardless of structure\n",
        "\n",
        "- ...which suggests that the intended length of those text fragments should be on the order of a paragraph (or similar, often-larger unbroken text chunk), \n",
        " - because it's easier to join later than to split later.\n",
        " - if you wantedsentence splitting, you might want to do that in your own post-processing; this seems out of scope for our cruder goals\n",
        "\n",
        "- metadata will probably \n",
        "  - not be things like header, section, paragraph, sentence\n",
        "  - be more like \"document seems to indicate this is three-deep, and the last header was 'intro'; do with that information what you will\"\n",
        "\n",
        "- for this to always be crude.\n",
        "  - Do not expect this to be very structured, unless you can restrict yourself to a document set that is uniform enough document format that the _output_ is  similarly uniform.\n",
        "  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random, pprint, collections\n",
        "\n",
        "import wetsuite.helpers.koop_parse\n",
        "import wetsuite.helpers.localdata\n",
        "import wetsuite.helpers.etree\n",
        "import wetsuite.helpers.escape\n",
        "import wetsuite.helpers.util\n",
        "import wetsuite.helpers.split\n",
        "import wetsuite.datasets\n",
        "import wetsuite.extras.pdf\n",
        "import wetsuite.helpers.notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Collect some varied documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pick out varied documents, to see how many we process decently\n",
        "example_docs = {} # someid -> docbytes\n",
        "\n",
        "if 0: # BWB XML\n",
        "    bwb  = wetsuite.datasets.load('bwb-mostrecent-xml')\n",
        "    for bwbid, docbytes in bwb.data.random_sample(100):\n",
        "        example_docs[bwbid] = docbytes\n",
        "\n",
        "if 0: # CVDR XML\n",
        "    cvdr = wetsuite.datasets.load('cvdr-mostrecent-xml')\n",
        "    for cvdrid, docbytes in cvdr.data.random_sample(100):\n",
        "        example_docs['cvdr:CVDR'+cvdrid] = docbytes\n",
        "\n",
        "#cvdr = wetsuite.datasets.load('cvdr-mostrecent-meta-struc')\n",
        "#for cvdrid, dd in cvdr.data.random_sample(100):\n",
        "#    pprint.pprint(dd)\n",
        "\n",
        "if 1: # CVDR HTML\n",
        "    cvdr = wetsuite.datasets.load('cvdr-mostrecent-html')\n",
        "    for cvdrid, docbytes in cvdr.data.random_sample(100):\n",
        "        example_docs['html:CVDR'+cvdrid] = docbytes\n",
        "\n",
        "if 0: # Rechtspraak XML\n",
        "    rechtspraak_xml = wetsuite.helpers.localdata.LocalKV('rechtspraak_fetched.db', key_type=str, value_type=bytes, read_only=True)\n",
        "    for rsurl, xmlbytes in rechtspraak_xml.random_sample(100):\n",
        "        example_docs[rsurl] = xmlbytes\n",
        "    # TODO:\n",
        "    #rechtspraak  = wetsuite.datasets.load('rechtspraaknl-xml')\n",
        "    ##for bwbid, docbytes in bwb.data.random_sample(100):\n",
        "    ##    example_docs[bwbid] = docbytes\n",
        "    #rechtspraak.data.random_choice()\n",
        "    #or maybe cache-fetching the URLs mentioned in rechtspraaknl-struc ?\n",
        "\n",
        "if 0: # Woo PDF ?\n",
        "    pass\n",
        "\n",
        "if 0: \n",
        "    # these should soon be datasets, but for now are internal collection stores\n",
        "    bus_data = wetsuite.helpers.localdata.LocalKV( 'bus_data.db', key_type=str, value_type=bytes )\n",
        "\n",
        "    for path in bus_data.random_keys(250000):\n",
        "        if '.xml' in path and random.uniform(0,1) < 0.5: # much of this is xml (data or metadata), try to bring up the HTML and PDF\n",
        "            continue\n",
        "        if 'gmb' in path and random.uniform(0,1) < 0.9: # roughly three quarters of this store is gmb, try to balance that a little bit\n",
        "            continue\n",
        "        bytedoc = bus_data.get( path )\n",
        "        example_docs[ path ] = bytedoc\n",
        "\n",
        "if 0: # some cherry-picked PDFs\n",
        "    example_docs[ 'simple2page' ] = wetsuite.helpers.net.download('https://zoek.officielebekendmakingen.nl/wsb-2022-9718.pdf')\n",
        "    example_docs[ '2023D51633' ] = wetsuite.helpers.net.download('https://www.tweedekamer.nl/downloads/document?id=2023D51633')\n",
        "    #example_docs[ 'stb-1952-10' ] = wetsuite.helpers.net.download('https://repository.overheid.nl/frbr/officielepublicaties/stb/1952/stb-1952-10/1/pdf/stb-1952-10.pdf')\n",
        "    #example_docs[ 'stb-1975-102' ] = wetsuite.helpers.net.download('https://repository.overheid.nl/frbr/officielepublicaties/stb/1975/stb-1975-102/1/pdf/stb-1975-102.pdf')\n",
        "    example_docs[ '3col' ] = wetsuite.helpers.net.download('https://zoek.officielebekendmakingen.nl/stcrt-1995-28-p9-SC1944.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### See what kind of mix of documents we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "edpairs = list( example_docs.items() )\n",
        "random.shuffle( edpairs )\n",
        "print( len(edpairs) )\n",
        "for key, by in edpairs:\n",
        "    print( '%-70s %s'%(key, by[:60]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### See what portion gets detected as something known\n",
        "\n",
        "No fragment output yet, just seeing how well we cover what we gave it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'reload' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreload\u001b[49m(wetsuite\u001b[38;5;241m.\u001b[39mhelpers\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m      2\u001b[0m reload(wetsuite\u001b[38;5;241m.\u001b[39mhelpers\u001b[38;5;241m.\u001b[39mutil)\n\u001b[1;32m      4\u001b[0m count \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mint\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reload' is not defined"
          ]
        }
      ],
      "source": [
        "#reload(wetsuite.helpers.split)\n",
        "#reload(wetsuite.helpers.util)\n",
        "\n",
        "count = collections.defaultdict(int)\n",
        "\n",
        "for someid,docbytes in wetsuite.helpers.notebook.ProgressBar( edpairs[:500] ):\n",
        "    # if docbytes.startswith( b'<?' ):\n",
        "    #     try:\n",
        "    #         wetsuite.helpers.etree.fromstring( docbytes )\n",
        "    #     except Exception as e:\n",
        "    #         badxmls.append( (e, someid) )\n",
        "\n",
        "    if 'metadata' in someid or 'changelog' in someid:\n",
        "        count['skipmeta'] += 1\n",
        "        continue\n",
        "\n",
        "    options = wetsuite.helpers.split.decide( docbytes )\n",
        "    #for option in options:\n",
        "    if len(options)==0:\n",
        "        print( '\\n------- %s -------'%someid )\n",
        "        print( 'NOTHING' )        \n",
        "        count['nothing'] += 1\n",
        "\n",
        "    else:\n",
        "        if options[0][0] > 100:\n",
        "            count['low'] += 1\n",
        "            #print( '\\n------- %s -------'%someid )\n",
        "            #print( 'ONLYLOW' )        \n",
        "            #for option in options:\n",
        "            #    print( '  ',option)\n",
        "        else:\n",
        "            count['good'] += 1\n",
        "\n",
        "count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### See what the splitter gives us\n",
        "\n",
        "...a few at a time, because otherwise this would be a _lot_ of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edpairs = list( example_docs.items() )\n",
        "#random.shuffle( edpairs )\n",
        "edpairs = sorted( edpairs )\n",
        "len(edpairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(wetsuite.helpers.split)\n",
        "reload(wetsuite.helpers.koop_parse)\n",
        "\n",
        "for key, docbytes in edpairs[:200]:\n",
        "    if 'metadata' in key: # we care about documents; skip metadata-only, in case the above accidentally filtered them in\n",
        "        continue\n",
        "\n",
        "    thresh = 500\n",
        "    splitters = wetsuite.helpers.split.decide(docbytes, thresh=thresh)\n",
        "    if len(splitters)==0:\n",
        "        print( 'WARN for  %-30s - no splitter says it applies  (under threshold %s)'%(key, thresh) )\n",
        "        # \"stop on the first problem case and display it so we can add it\" style logic:\n",
        "        if b'<html' in docbytes:\n",
        "            display( wetsuite.helpers.etree.debug_color(docbytes) )\n",
        "            break\n",
        "        continue\n",
        "    \n",
        "    for score, fragproc in splitters: # use each processor that said they would be useful\n",
        "        frags = fragproc.fragments()\n",
        "        if len(frags)==0:\n",
        "            print( 'WARN for  %-30s - no output from splitter %s'%(key, type(fragproc).__name__ ) )\n",
        "            #if 'html.zip' in key:\n",
        "            #    print(wetsuite.helpers.util.get_ziphtml(docbytes))\n",
        "            #    print( fragproc.soup )\n",
        "        else:\n",
        "            textlen = sum(  list(len(fragtext)  for _,_,fragtext in frags))\n",
        "            print( f'INFO for  {key:^30s} - {type(fragproc).__name__} gave {textlen} chars of text' )\n",
        "\n",
        "            #for o1,o2,o3 in frags:\n",
        "            #    print( type(o2) )\n",
        "\n",
        "            if 1: # you may want to disable this print-everything when doing \"are we missing anything\" debug\n",
        "                #display( wetsuite.helpers.etree.debug_color(docbytes) )\n",
        "                display( wetsuite.helpers.split.SplitDebug( frags ) )\n",
        "                break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20247"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(  list(len(fragtext)  for _,_,fragtext in frags)  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
